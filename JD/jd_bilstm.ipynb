{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import data_preprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据划分为训练集和测试集\n",
    "X_train, X_test, Y_train, Y_test = data_preprocess.tensorFromData()\n",
    "trainDataSet = data_preprocess.TextDataSet(X_train, Y_train)\n",
    "testDataSet = data_preprocess.TextDataSet(X_test, Y_test)\n",
    "trainDataLoader = DataLoader(trainDataSet, batch_size=16, shuffle=True)\n",
    "testDataLoader = DataLoader(testDataSet, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取字典\n",
    "word_to_inx, inx_to_word = data_preprocess.get_dic()\n",
    "len_dic = len(word_to_inx)\n",
    "\n",
    "# 定义超参数\n",
    "MAXLEN = 64\n",
    "input_dim = MAXLEN\n",
    "emb_dim = 128\n",
    "num_epoches = 20\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义模型\n",
    "class CNN_BiLSTM_Concat_model(nn.Module):\n",
    "    def __init__(self, len_dic, emb_dim, input_dim):\n",
    "        super(CNN_BiLSTM_Concat_model, self).__init__()\n",
    "        self.embed = nn.Embedding(len_dic, emb_dim)  # # batchsize,64（序列长）,128(词向量长度)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 256, 3, 1, 1),  # b,256,128\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2, 2)  # b,256,64 ->b,256*64\n",
    "        )\n",
    "        self.linear_cnn=nn.Linear(256*64,256)#b,256\n",
    "        #64,b,128\n",
    "        self.bilstm = nn.LSTM(input_size=128, hidden_size=256, dropout=0.2,bidirectional=True) # 64,b,256*2 ->b,64,256*2 ->b,64*256*2\n",
    "        self.linear_lstm=nn.Linear(64*256*2,256)  #b,256\n",
    "        #b,256+256\n",
    "        self.classify = nn.Linear(256+256, 3)  # b,3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.embed(x)\n",
    "        out1= self.conv(x)\n",
    "        b,c,d=out1.size()\n",
    "        out1=out1.view(b,c*d)\n",
    "        out1=self.linear_cnn(out1)\n",
    "\n",
    "        x=x.permute(1,0,2)\n",
    "        out2,_=self.bilstm(x)\n",
    "        out2=out2.permute(1,0,2).contiguous()\n",
    "        b,c,d=out2.size()\n",
    "        out2=out2.view(b,c*d)\n",
    "        out2=self.linear_lstm(out2)\n",
    "\n",
    "        out = torch.cat((out1,out2),1)\n",
    "        # print(out.size())\n",
    "        out = self.classify(out)\n",
    "        # print(out.size())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/786],train loss is:1.894302,train acc is:0.484848\n",
      "[199/786],train loss is:1.394250,train acc is:0.530151\n",
      "[299/786],train loss is:1.206809,train acc is:0.557483\n",
      "[399/786],train loss is:1.110612,train acc is:0.570645\n",
      "[499/786],train loss is:1.044849,train acc is:0.587174\n",
      "[599/786],train loss is:0.998292,train acc is:0.599228\n",
      "[699/786],train loss is:0.961464,train acc is:0.607922\n",
      "epoch:[0],train loss is:0.935291,train acc is:0.613391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss is:0.828618,test acc is:0.626586\n",
      "best acc is 0.626586,best model is changed\n",
      "[99/786],train loss is:0.594188,train acc is:0.761364\n",
      "[199/786],train loss is:0.604486,train acc is:0.750628\n",
      "[299/786],train loss is:0.600047,train acc is:0.747701\n",
      "[399/786],train loss is:0.594671,train acc is:0.748434\n",
      "[499/786],train loss is:0.599063,train acc is:0.742986\n",
      "[599/786],train loss is:0.605668,train acc is:0.741235\n",
      "[699/786],train loss is:0.610550,train acc is:0.737303\n",
      "epoch:[1],train loss is:0.610173,train acc is:0.733063\n",
      "test loss is:0.877052,test acc is:0.626904\n",
      "best acc is 0.626904,best model is changed\n",
      "[99/786],train loss is:0.420149,train acc is:0.842803\n",
      "[199/786],train loss is:0.407925,train acc is:0.833543\n",
      "[299/786],train loss is:0.409311,train acc is:0.832776\n",
      "[399/786],train loss is:0.414454,train acc is:0.828634\n",
      "[499/786],train loss is:0.418837,train acc is:0.825401\n",
      "[599/786],train loss is:0.421121,train acc is:0.824082\n",
      "[699/786],train loss is:0.418821,train acc is:0.825554\n",
      "epoch:[2],train loss is:0.420024,train acc is:0.823155\n",
      "test loss is:0.835797,test acc is:0.669099\n",
      "best acc is 0.669099,best model is changed\n",
      "[99/786],train loss is:0.248955,train acc is:0.909091\n",
      "[199/786],train loss is:0.242807,train acc is:0.915201\n",
      "[299/786],train loss is:0.241216,train acc is:0.911371\n",
      "[399/786],train loss is:0.247598,train acc is:0.905858\n",
      "[499/786],train loss is:0.254704,train acc is:0.901177\n",
      "[599/786],train loss is:0.253685,train acc is:0.901503\n",
      "[699/786],train loss is:0.260374,train acc is:0.898963\n",
      "epoch:[3],train loss is:0.261402,train acc is:0.896390\n",
      "test loss is:1.172353,test acc is:0.672272\n",
      "best acc is 0.672272,best model is changed\n",
      "[99/786],train loss is:0.126293,train acc is:0.963384\n",
      "[199/786],train loss is:0.137918,train acc is:0.951319\n",
      "[299/786],train loss is:0.139425,train acc is:0.948579\n",
      "[399/786],train loss is:0.151921,train acc is:0.944706\n",
      "[499/786],train loss is:0.166422,train acc is:0.939254\n",
      "[599/786],train loss is:0.175011,train acc is:0.935935\n",
      "[699/786],train loss is:0.183344,train acc is:0.933208\n",
      "epoch:[4],train loss is:0.185163,train acc is:0.929707\n",
      "test loss is:1.634772,test acc is:0.649746\n",
      "[99/786],train loss is:0.109204,train acc is:0.971591\n",
      "[199/786],train loss is:0.101513,train acc is:0.971106\n",
      "[299/786],train loss is:0.106184,train acc is:0.966137\n",
      "[399/786],train loss is:0.108994,train acc is:0.966009\n",
      "[499/786],train loss is:0.117337,train acc is:0.963051\n",
      "[599/786],train loss is:0.119667,train acc is:0.961498\n",
      "[699/786],train loss is:0.129825,train acc is:0.957439\n",
      "epoch:[5],train loss is:0.135681,train acc is:0.953244\n",
      "test loss is:1.952552,test acc is:0.662119\n",
      "[99/786],train loss is:0.107155,train acc is:0.972854\n",
      "[199/786],train loss is:0.107529,train acc is:0.968907\n",
      "[299/786],train loss is:0.106807,train acc is:0.967182\n",
      "[399/786],train loss is:0.115998,train acc is:0.965382\n",
      "[499/786],train loss is:0.113316,train acc is:0.966558\n",
      "[599/786],train loss is:0.120756,train acc is:0.963898\n",
      "[699/786],train loss is:0.123376,train acc is:0.962536\n",
      "epoch:[6],train loss is:0.122943,train acc is:0.959685\n",
      "test loss is:2.417047,test acc is:0.640863\n",
      "[99/786],train loss is:0.071948,train acc is:0.988005\n",
      "[199/786],train loss is:0.069018,train acc is:0.981784\n",
      "[299/786],train loss is:0.069252,train acc is:0.979515\n",
      "[399/786],train loss is:0.064027,train acc is:0.979950\n",
      "[499/786],train loss is:0.077606,train acc is:0.977705\n",
      "[599/786],train loss is:0.085243,train acc is:0.975271\n",
      "[699/786],train loss is:0.085607,train acc is:0.974160\n",
      "epoch:[7],train loss is:0.088593,train acc is:0.970658\n",
      "test loss is:3.014137,test acc is:0.624048\n",
      "[99/786],train loss is:0.064040,train acc is:0.989268\n",
      "[199/786],train loss is:0.072994,train acc is:0.984925\n",
      "[299/786],train loss is:0.069041,train acc is:0.982651\n",
      "[399/786],train loss is:0.068461,train acc is:0.982769\n",
      "[499/786],train loss is:0.079795,train acc is:0.978457\n",
      "[599/786],train loss is:0.086013,train acc is:0.976523\n",
      "[699/786],train loss is:0.086033,train acc is:0.975948\n",
      "epoch:[8],train loss is:0.087607,train acc is:0.972885\n",
      "test loss is:2.888920,test acc is:0.646891\n",
      "[99/786],train loss is:0.169386,train acc is:0.974116\n",
      "[199/786],train loss is:0.134949,train acc is:0.971734\n",
      "[299/786],train loss is:0.131917,train acc is:0.970109\n",
      "[399/786],train loss is:0.119452,train acc is:0.971178\n",
      "[499/786],train loss is:0.112843,train acc is:0.972320\n",
      "[599/786],train loss is:0.112530,train acc is:0.971619\n",
      "[699/786],train loss is:0.111152,train acc is:0.971745\n",
      "epoch:[9],train loss is:0.107156,train acc is:0.969784\n",
      "test loss is:3.279069,test acc is:0.648477\n",
      "[99/786],train loss is:0.041487,train acc is:0.996843\n",
      "[199/786],train loss is:0.036263,train acc is:0.994661\n",
      "[299/786],train loss is:0.035780,train acc is:0.993520\n",
      "[399/786],train loss is:0.038646,train acc is:0.991541\n",
      "[499/786],train loss is:0.045382,train acc is:0.989479\n",
      "[599/786],train loss is:0.042814,train acc is:0.989775\n",
      "[699/786],train loss is:0.042255,train acc is:0.989270\n",
      "epoch:[10],train loss is:0.043846,train acc is:0.985767\n",
      "test loss is:3.556297,test acc is:0.645305\n",
      "[99/786],train loss is:0.026247,train acc is:1.001263\n",
      "[199/786],train loss is:0.043087,train acc is:0.993090\n",
      "[299/786],train loss is:0.040952,train acc is:0.992057\n",
      "[399/786],train loss is:0.038486,train acc is:0.991541\n",
      "[499/786],train loss is:0.042894,train acc is:0.990731\n",
      "[599/786],train loss is:0.050919,train acc is:0.989253\n",
      "[699/786],train loss is:0.049132,train acc is:0.988913\n",
      "epoch:[11],train loss is:0.052561,train acc is:0.985369\n",
      "test loss is:3.711804,test acc is:0.644036\n",
      "[99/786],train loss is:0.197868,train acc is:0.976010\n",
      "[199/786],train loss is:0.160423,train acc is:0.977387\n",
      "[299/786],train loss is:0.130973,train acc is:0.979097\n",
      "[399/786],train loss is:0.116841,train acc is:0.979637\n",
      "[499/786],train loss is:0.113721,train acc is:0.979208\n",
      "[599/786],train loss is:0.114507,train acc is:0.977254\n",
      "[699/786],train loss is:0.110778,train acc is:0.977468\n",
      "epoch:[12],train loss is:0.109765,train acc is:0.974237\n",
      "test loss is:3.526681,test acc is:0.644036\n",
      "[99/786],train loss is:0.028526,train acc is:1.001894\n",
      "[199/786],train loss is:0.053435,train acc is:0.991834\n",
      "[299/786],train loss is:0.050352,train acc is:0.991221\n",
      "[399/786],train loss is:0.056254,train acc is:0.988878\n",
      "[499/786],train loss is:0.061530,train acc is:0.987099\n",
      "[599/786],train loss is:0.061620,train acc is:0.986749\n",
      "[699/786],train loss is:0.059794,train acc is:0.986320\n",
      "epoch:[13],train loss is:0.057635,train acc is:0.984017\n",
      "test loss is:3.686037,test acc is:0.646891\n",
      "[99/786],train loss is:0.015098,train acc is:1.003788\n",
      "[199/786],train loss is:0.018951,train acc is:0.997802\n",
      "[299/786],train loss is:0.023678,train acc is:0.995192\n",
      "[399/786],train loss is:0.025570,train acc is:0.994204\n",
      "[499/786],train loss is:0.027918,train acc is:0.993612\n",
      "[599/786],train loss is:0.033702,train acc is:0.991966\n",
      "[699/786],train loss is:0.050991,train acc is:0.988287\n",
      "epoch:[14],train loss is:0.063391,train acc is:0.983381\n",
      "test loss is:5.651579,test acc is:0.644353\n",
      "[99/786],train loss is:0.123160,train acc is:0.981692\n",
      "[199/786],train loss is:0.111222,train acc is:0.980214\n",
      "[299/786],train loss is:0.112221,train acc is:0.979724\n",
      "[399/786],train loss is:0.118810,train acc is:0.976974\n",
      "[499/786],train loss is:0.112568,train acc is:0.977580\n",
      "[599/786],train loss is:0.116214,train acc is:0.976106\n",
      "[699/786],train loss is:0.113453,train acc is:0.975680\n",
      "epoch:[15],train loss is:0.110029,train acc is:0.973521\n",
      "test loss is:4.473319,test acc is:0.652919\n",
      "[99/786],train loss is:0.036890,train acc is:0.996212\n",
      "[199/786],train loss is:0.047773,train acc is:0.991834\n",
      "[299/786],train loss is:0.047179,train acc is:0.991221\n",
      "[399/786],train loss is:0.056413,train acc is:0.988252\n",
      "[499/786],train loss is:0.063769,train acc is:0.987224\n",
      "[599/786],train loss is:0.065761,train acc is:0.986227\n",
      "[699/786],train loss is:0.069890,train acc is:0.985783\n",
      "epoch:[16],train loss is:0.068317,train acc is:0.983222\n",
      "test loss is:5.237228,test acc is:0.648477\n",
      "[99/786],train loss is:0.037522,train acc is:0.998106\n",
      "[199/786],train loss is:0.045806,train acc is:0.993090\n",
      "[299/786],train loss is:0.046652,train acc is:0.991221\n",
      "[399/786],train loss is:0.042369,train acc is:0.991228\n",
      "[499/786],train loss is:0.046114,train acc is:0.990731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/786],train loss is:0.043677,train acc is:0.990818\n",
      "[699/786],train loss is:0.046979,train acc is:0.990075\n",
      "epoch:[17],train loss is:0.048095,train acc is:0.987198\n",
      "test loss is:4.547723,test acc is:0.645622\n",
      "[99/786],train loss is:0.026183,train acc is:0.999369\n",
      "[199/786],train loss is:0.021360,train acc is:0.996545\n",
      "[299/786],train loss is:0.028130,train acc is:0.994147\n",
      "[399/786],train loss is:0.032862,train acc is:0.992481\n",
      "[499/786],train loss is:0.037034,train acc is:0.991984\n",
      "[599/786],train loss is:0.039036,train acc is:0.991131\n",
      "[699/786],train loss is:0.042473,train acc is:0.989986\n",
      "epoch:[18],train loss is:0.042539,train acc is:0.987675\n",
      "test loss is:5.601687,test acc is:0.643084\n",
      "[99/786],train loss is:0.043565,train acc is:0.999369\n",
      "[199/786],train loss is:0.038253,train acc is:0.994661\n",
      "[299/786],train loss is:0.038100,train acc is:0.992684\n",
      "[399/786],train loss is:0.033750,train acc is:0.992638\n",
      "[499/786],train loss is:0.044056,train acc is:0.991733\n",
      "[599/786],train loss is:0.041106,train acc is:0.991757\n",
      "[699/786],train loss is:0.041995,train acc is:0.991059\n",
      "epoch:[19],train loss is:0.040998,train acc is:0.988947\n",
      "test loss is:4.433057,test acc is:0.639594\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    model = CNN_BiLSTM_Concat_model(len_dic, emb_dim, input_dim).cuda()\n",
    "else:\n",
    "    model = CNN_BiLSTM_Concat_model(len_dic, emb_dim, input_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimzier = optim.Adam(model.parameters(), lr=1e-3)\n",
    "best_acc = 0\n",
    "best_model = None\n",
    "for epoch in range(num_epoches):# 数据集迭代 https://www.jianshu.com/p/043083d114d4\n",
    "    #初始化loss和acc\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "#     通过 module.train() 和 module.eval() 来切换模型的 训练测试阶段。\n",
    "    model.train()\n",
    "    \n",
    "    #steps print\n",
    "    for i, data in enumerate(trainDataLoader):\n",
    "        x, y = data\n",
    "        if use_cuda:\n",
    "            x, y = Variable(x).cuda(), Variable(y).cuda()\n",
    "        else:\n",
    "            x, y = Variable(x), Variable(y)\n",
    "            \n",
    "        # forward\n",
    "        out = model(x)#预测\n",
    "        loss = criterion(out, y)#计算loss\n",
    "        \n",
    "        train_loss += loss.item() * len(y)\n",
    "        _, pre = torch.max(out, 1)\n",
    "        num_acc = (pre == y).sum()\n",
    "        train_acc += num_acc.item()\n",
    "        # backward\n",
    "        optimzier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzier.step()\n",
    "        if (i + 1) % 100 == 0:#每100步计算精度\n",
    "            print('[{}/{}],train loss is:{:.6f},train acc is:{:.6f}'.format(i, len(trainDataLoader),\n",
    "                                                                            train_loss / (i * batch_size),\n",
    "                                                                            train_acc / (i * batch_size)))\n",
    "    print(\n",
    "        'epoch:[{}],train loss is:{:.6f},train acc is:{:.6f}'.format(epoch,\n",
    "                                                                     train_loss / (len(trainDataLoader) * batch_size),\n",
    "                                                                     train_acc / (len(trainDataLoader) * batch_size)))\n",
    "    #eval()时,框架会自动把BN和DropOut固定住,不会取平均,而是用训练好的值\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for i, data in enumerate(testDataLoader):\n",
    "        x, y = data\n",
    "        if use_cuda:\n",
    "            x = Variable(x, volatile=True).cuda()\n",
    "            y = Variable(y, volatile=True).cuda()\n",
    "        else:\n",
    "            x = Variable(x, volatile=True)\n",
    "            y = Variable(y, volatile=True)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        eval_loss += loss.item() * len(y)\n",
    "        _, pre = torch.max(out, 1)\n",
    "        num_acc = (pre == y).sum()\n",
    "        eval_acc += num_acc.item()\n",
    "    print('test loss is:{:.6f},test acc is:{:.6f}'.format(\n",
    "        eval_loss / (len(testDataLoader) * batch_size),\n",
    "        eval_acc / (len(testDataLoader) * batch_size)))\n",
    "    if best_acc<(eval_acc / (len(testDataLoader) * batch_size)):\n",
    "        best_acc=eval_acc / (len(testDataLoader) * batch_size)\n",
    "        best_model=model.state_dict()\n",
    "        # print(best_model)\n",
    "        print('best acc is {:.6f},best model is changed'.format(best_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
